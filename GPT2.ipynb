{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFFpYjAdsf7se6i0XTX5jK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisiitoDev/LLMs/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0apXfIq4Ntri",
        "outputId": "0554c39a-b183-4c64-e8be-d0f721e67708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting np\n",
            "  Downloading np-1.0.2.tar.gz (7.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (2.0.2)\n",
            "Collecting torch==2.7.0 (from xformers)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (4.14.0)\n",
            "Collecting sympy>=1.13.3 (from torch==2.7.0->xformers)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->xformers)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->xformers)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->xformers) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->xformers) (3.0.2)\n",
            "Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: np\n",
            "  Building wheel for np (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np: filename=np-1.0.2-py3-none-any.whl size=13657 sha256=6aafe8d9ec4f1d8fabbca7e067d540bcc9d2830df77e2e36b518c169645f29d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/20/42/6ee214e617f78123903f603524d662ac6fa14154c3027fd992\n",
            "Successfully built np\n",
            "Installing collected packages: nvidia-cusparselt-cu12, np, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed np-1.0.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0 xformers-0.0.30\n"
          ]
        }
      ],
      "source": [
        "!pip install einops xformers np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules import ModuleList\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "from torch import nn\n",
        "\n",
        "import copy\n",
        "import math\n"
      ],
      "metadata": {
        "id": "1H9joc7mOBb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "vdNjSMNeQDpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTw3uPdXQtht",
        "outputId": "227abfb4-e579-4148-f9e9-aa9ed62611d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_clones(module, n):\n",
        "  return ModuleList([copy.deepcopy(module) for i in range(n)])"
      ],
      "metadata": {
        "id": "DILz6t6oQR1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1D(nn.Module):\n",
        "  def __init__(self, nx, nf):\n",
        "    '''\n",
        "    nx: Numero de datos de entrada\n",
        "    nf: Numero de filtros. (Canales de salida).\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.nf = nf\n",
        "    #Inicializando matriz vacia de pesos del tamaño (nx)X(nf)\n",
        "    w = torch.empty(nx,nf)\n",
        "    #Calculando los pesos con una distribución normal.\n",
        "    nn.init.normal_(w, std=0.02)\n",
        "    #Calculando los pesos y sesgos encodeandos usando nn.Parameter.\n",
        "    self.weight = nn.Parameter(w)\n",
        "    self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''x:Tensor de entrada.'''\n",
        "    #El tamaño de la salida es la suma de la segunda dimensionm de X y el numero de filtros nf.\n",
        "    size_out = x.size()[:-1] + (self.nf,)\n",
        "    #Producto punto Q,K(Transpuesta) y V\n",
        "    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight) # x.view ayuda a calcular la transpuesta.\n",
        "    x = x.view(*size_out)\n",
        "    return x"
      ],
      "metadata": {
        "id": "lRVYy-4DQd_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dropout, d_model=768, nx=768*4):\n",
        "    super().__init__()\n",
        "    self.c_fc     = Conv1D(d_model, nx)\n",
        "    self.c_proj   = Conv1D(nx, d_model)\n",
        "    self.act      = F.gelu\n",
        "    self.dropout  = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
      ],
      "metadata": {
        "id": "5FYdPV-xTlMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
        "    '''Función de construcción\n",
        "    Params:\n",
        "    d_model: Dimensión que necesita ser ingresada en el modelo.\n",
        "    n_head: La cantidad de heads de atención\n",
        "    n_ctx: Buffer para guardar los registros del sesgo.\n",
        "    scale: Escalar y estabilidad númerica (sqrt(dk))\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.n_head = n_head\n",
        "    self.d_model = d_model\n",
        "    self.c_attn = Conv1D(d_model, d_model*3)\n",
        "    self.scale = scale\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.c_proj = Conv1D(d_model, d_model)\n",
        "\n",
        "  def split_heads(self,x):\n",
        "    \"\"\"\n",
        "    Dividiend en la cantidad de heads y retornando.\n",
        "    return shape ['Barch', 'head', 'sequence', 'features']\n",
        "    \"\"\"\n",
        "    new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n",
        "    x = x.view(*new_shape)\n",
        "    return x.permute(0,2,1,3)\n",
        "\n",
        "  def _attn(self, q, k, v, attn_mask=None):\n",
        "    \"\"\"Función de atención principal.\n",
        "    Que calcula usando la formula de producto de punto de atención.\"\"\"\n",
        "    scores = torch.matmul(q, k.transpose(-2,-1)) #producto punto de Q*K(t)\n",
        "    if self.scale: scores = scores/math.sqrt(v.size(-1)) #escalandolo por sqrt(dk)\n",
        "    nd, ns = scores.size(-2), scores.size(-1)\n",
        "    if attn_mask is not None: scores = scores + attn_mask\n",
        "    scores = self.softmax(scores)\n",
        "    scores = self.dropout(scores)\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output\n",
        "\n",
        "  def merge_heads(self,x):\n",
        "    x = x.permute(0,2,1,3).contiguous()\n",
        "    new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
        "    return x.view(*new_shape)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Función de para calcular atención, separar los heads y combinarlos de nuevos.'''\n",
        "    x = self.c_attn(x)\n",
        "    query, key, value = x.split(self.d_model, dim=2)\n",
        "    query, key, value = self.split_heads(query), self.split_heads(key), self.split_heads(value)\n",
        "    out = self._attn(query, key, value)\n",
        "    out = self.merge_heads(out)\n",
        "    out = self.c_proj(out)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "URktB2fOUj_N"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attn = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
        "    self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
        "    self.ln_1 = LayerNorm(d_model)\n",
        "    self.ln_2 = LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.feedforward(self.ln_2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "L6R7FKVNcoaP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "  def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vob_size=50257):\n",
        "    '''nlayer: La cantidad de veces que queremos multiplicar el transformer.\n",
        "    n_ctx: El contexto, la cantidad total de tokens que puede ver en el pasado de las palabras.\n",
        "    d_model: Dimesiones del modelo\n",
        "    vob_size: Tamaño del vocabulario usado en el entrenamiento.'''\n",
        "    super(GPT2, self).__init__()\n",
        "    self.nlayers = nlayers\n",
        "    block = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
        "    self.h = _get_clones(block, 12)\n",
        "    self.wte = nn.Embedding(vob_size, d_model)\n",
        "    self.wpe = nn.Embedding(n_ctx, d_model)\n",
        "    self.drop = nn.Dropout(0.1)\n",
        "    self.ln_f = LayerNorm(d_model)\n",
        "    self.out = nn.Linear(d_model, vob_size, bias=False)\n",
        "    self.loss_fn = nn.CrossEntropyLoss()\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    '''Incilizacion de los pesos'''\n",
        "    self.out.weight = self.wte.weight\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    '''Iniciliazación con la medida y S.D.'''\n",
        "    if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
        "      module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "      if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
        "        '''Data Bias Zero'''\n",
        "        module.bias.data.zero_()\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "      module.bias.data.zero_()\n",
        "      module.weight.data.fill_(1.0)\n",
        "\n",
        "  def forward(self, src, labels=None, pos_ids=None):\n",
        "    '''Añadir el embedding posicional, dropping y añadiendo los inputs usados por la función\n",
        "    de perdida y finalmente añadiendo la salida y la peridida.'''\n",
        "\n",
        "    if pos_ids is None:\n",
        "      pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
        "\n",
        "    pos_ids = pos_ids.to(src.device)\n",
        "\n",
        "    inp = self.drop((self.wte(src) + self.wpe(pos_ids)))\n",
        "    for i in range(self.nlayers): inp = self.h[i](inp)\n",
        "\n",
        "    inp = self.ln_f(inp)\n",
        "    logits = self.out(inp)\n",
        "\n",
        "    outputs = (logits,) + (inp,)\n",
        "\n",
        "    if labels is not None:\n",
        "      shift_logits = logits[..., :-1, :].contiguous()\n",
        "      shift_labels = labels[..., 1:].contiguous()\n",
        "      loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "      outputs = (loss,) + outputs\n",
        "      return loss.mean()\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "Jjdyk56Dd2mO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import time\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "-Vqdpa3NcCEN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2()"
      ],
      "metadata": {
        "id": "xrGe9nTOcNDJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRBmxAxEO-qM",
        "outputId": "b62a860d-394b-4ce3-c576-3f04966f3465"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  522M  100  522M    0     0  41.9M      0  0:00:12  0:00:12 --:--:-- 55.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = model.state_dict()\n",
        "state_dict = torch.load(\"./gpt2-pytorch_model.bin\", weights_only=False)\n",
        "\n",
        "old_keys = []\n",
        "new_keys = []\n",
        "\n",
        "for key in state_dict.keys():\n",
        "  if \"mlp\" in key:\n",
        "    new_key = key.replace(\"mlp\", \"feedforward\")\n",
        "    new_keys.append(new_key);\n",
        "    old_keys.append(key)"
      ],
      "metadata": {
        "id": "Zu7FSw59P66g"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for old_key, new_key in zip(old_keys, new_keys):\n",
        "  state_dict[new_key] = state_dict.pop(old_key)"
      ],
      "metadata": {
        "id": "52nlIepURqb2"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drCQNrZRR9BZ",
        "outputId": "18ed8152-d161-42b0-e5de-ef8f25366516"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2(\n",
              "  (h): ModuleList(\n",
              "    (0-11): 12 x TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(1024, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  (loss_fn): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "id": "lHYaNEzjSiM1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_bytes = total_params * 4\n",
        "size_mb = size_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"El tamaño total de GPT2 sin alteraciones es: {size_bytes} bytes o {size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cZAER_7SwpT",
        "outputId": "6e181722-49f4-44e2-8134-0171f58a1aba"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tamaño total de GPT2 sin alteraciones es: 497759232 bytes o 474.70 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "context = torch.tensor([tokenizer.encode(\"The planet earth is a beautiful\")])"
      ],
      "metadata": {
        "id": "ZkCasin7TVd8"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(context, ntok=550):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for _ in range(ntok):\n",
        "    out = model(context)\n",
        "    logits = out[0][:, -1, :]\n",
        "    indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
        "    logits[indices_to_remove] = -np.inf\n",
        "    next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
        "    context = torch.cat([context, next_tok.unsqueeze(-1)], dim=1)\n",
        "\n",
        "  end_time = time.time()\n",
        "  inference_time = end_time - start_time\n",
        "  return context, inference_time"
      ],
      "metadata": {
        "id": "5rhMoFfLT_9a"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out, inference_time = generate(context, ntok=40)\n",
        "decoded_output = tokenizer.decode(out[0])"
      ],
      "metadata": {
        "id": "5A6-QHvkVOIY"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "print(f\"Generated Output: {decoded_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFeiTaCoWZg5",
        "outputId": "2578339e-f464-4b10-b875-a48d8b693412"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time: 7.3275 seconds\n",
            "Generated Output: The planet earth is a beautiful part\" (and a is an ) is \"a\" and (\") is a \"a very different,\" a is an Earth \" earth, is earth). \"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "08P42tpNY_rS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}